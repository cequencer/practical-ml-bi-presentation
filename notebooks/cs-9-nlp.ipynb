{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "# increase default figure and font sizes for easier viewing\n",
    "plt.rcParams['figure.figsize'] = (8, 6)\n",
    "plt.rcParams['font.size'] = 14"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Processing (NLP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Adapted from [NLP Crash Course](http://files.meetup.com/7616132/DC-NLP-2013-09%20Charlie%20Greenbacker.pdf) by Charlie Greenbacker and [Introduction to NLP](http://spark-public.s3.amazonaws.com/nlp/slides/intro.pdf) by Dan Jurafsky*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is NLP?\n",
    "\n",
    "- Using computers to process (analyze, understand, generate) natural human languages\n",
    "- Most knowledge created by humans is unstructured text, and we need a way to make sense of it\n",
    "- Build probabilistic model using data about a language"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What are some of the higher level task areas?\n",
    "\n",
    "- **Information retrieval**: Find relevant results and similar results\n",
    "    - [Google](https://www.google.com/)\n",
    "- **Information extraction**: Structured information from unstructured documents\n",
    "    - [Events from Gmail](https://support.google.com/calendar/answer/6084018?hl=en)\n",
    "- **Machine translation**: One language to another\n",
    "    - [Google Translate](https://translate.google.com/)\n",
    "- **Text simplification**: Preserve the meaning of text, but simplify the grammar and vocabulary\n",
    "    - [Rewordify](https://rewordify.com/)\n",
    "    - [Simple English Wikipedia](https://simple.wikipedia.org/wiki/Main_Page)\n",
    "- **Predictive text input**: Faster or easier typing\n",
    "    - [My application](https://justmarkham.shinyapps.io/textprediction/)\n",
    "    - [A much better application](https://farsite.shinyapps.io/swiftkey-cap/)\n",
    "- **Sentiment analysis**: Attitude of speaker\n",
    "    - [Hater News](http://haternews.herokuapp.com/)\n",
    "- **Automatic summarization**: Extractive or abstractive summarization\n",
    "    - [autotldr](https://www.reddit.com/r/technology/comments/35brc8/21_million_people_still_use_aol_dialup/cr2zzj0)\n",
    "- **Natural Language Generation**: Generate text from data\n",
    "    - [How a computer describes a sports match](http://www.bbc.com/news/technology-34204052)\n",
    "    - [Publishers withdraw more than 120 gibberish papers](http://www.nature.com/news/publishers-withdraw-more-than-120-gibberish-papers-1.14763)\n",
    "- **Speech recognition and generation**: Speech-to-text, text-to-speech\n",
    "    - [Google's Web Speech API demo](https://www.google.com/intl/en/chrome/demos/speech.html)\n",
    "    - [Vocalware Text-to-Speech demo](https://www.vocalware.com/index/demo)\n",
    "- **Question answering**: Determine the intent of the question, match query with knowledge base, evaluate hypotheses\n",
    "    - [How did supercomputer Watson beat Jeopardy champion Ken Jennings?](http://blog.ted.com/how-did-supercomputer-watson-beat-jeopardy-champion-ken-jennings-experts-discuss/)\n",
    "    - [IBM's Watson Trivia Challenge](http://www.nytimes.com/interactive/2010/06/16/magazine/watson-trivia-game.html)\n",
    "    - [The AI Behind Watson](http://www.aaai.org/Magazine/Watson/watson.php)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What are some of the lower level components?\n",
    "\n",
    "- **Tokenization**: breaking text into tokens (words, sentences, n-grams)\n",
    "- **Stopword removal**: a/an/the\n",
    "- **Stemming and lemmatization**: root word\n",
    "- **TF-IDF**: word importance\n",
    "- **Part-of-speech tagging**: noun/verb/adjective\n",
    "- **Named entity recognition**: person/organization/location\n",
    "- **Spelling correction**: \"New Yrok City\"\n",
    "- **Word sense disambiguation**: \"buy a mouse\"\n",
    "- **Segmentation**: \"New York City subway\"\n",
    "- **Language detection**: \"translate this page\"\n",
    "- **Machine learning**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why is NLP hard?\n",
    "\n",
    "- **Ambiguity**:\n",
    "    - Hospitals are Sued by 7 Foot Doctors\n",
    "    - Juvenile Court to Try Shooting Defendant\n",
    "    - Local High School Dropouts Cut in Half\n",
    "- **Non-standard English**: text messages\n",
    "- **Idioms**: \"throw in the towel\"\n",
    "- **Newly coined words**: \"retweet\"\n",
    "- **Tricky entity names**: \"Where is A Bug's Life playing?\"\n",
    "- **World knowledge**: \"Mary and Sue are sisters\", \"Mary and Sue are mothers\"\n",
    "\n",
    "NLP requires an understanding of the **language** and the **world**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading in the Yelp Reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- \"corpus\" = collection of documents\n",
    "- \"corpora\" = plural form of corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#!pip install textblob\n",
    "#!python -m textblob.download_corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "from textblob import TextBlob, Word\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# read yelp.csv into a DataFrame\n",
    "url = '../data/yelp.csv'\n",
    "yelp = pd.read_csv(url)\n",
    "\n",
    "# create a new DataFrame that only contains the 5-star and 1-star reviews\n",
    "yelp_best_worst = yelp[(yelp.stars==5) | (yelp.stars==1)]\n",
    "\n",
    "# define X and y\n",
    "X = yelp_best_worst.text\n",
    "y = yelp_best_worst.stars\n",
    "\n",
    "# split the new DataFrame into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the [scikit-learn documentation](http://scikit-learn.org/stable/modules/feature_extraction.html#text-feature-extraction):\n",
    "\n",
    "> Text Analysis is a major application field for machine learning algorithms. However the raw data, a sequence of symbols cannot be fed directly to the algorithms themselves as most of them expect **numerical feature vectors with a fixed size** rather than the **raw text documents with variable length**.\n",
    "\n",
    "We will use [CountVectorizer](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) to \"convert text into a matrix of token counts\":"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **What:** Separate text into units such as sentences or words\n",
    "- **Why:** Gives structure to previously unstructured text\n",
    "- **Notes:** Relatively easy with English language text, not easy with some languages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cab</th>\n",
       "      <th>call</th>\n",
       "      <th>me</th>\n",
       "      <th>please</th>\n",
       "      <th>tonight</th>\n",
       "      <th>you</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   cab  call  me  please  tonight  you\n",
       "0    0     1   0       0        1    1\n",
       "1    1     1   1       0        0    0\n",
       "2    0     1   1       2        0    0"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# start with a simple example\n",
    "simple_train = [\n",
    "    'call you tonight',\n",
    "    'Call me a cab',\n",
    "    'please call me... PLEASE!'\n",
    "]\n",
    "# learn the 'vocabulary' of the training data\n",
    "vect = CountVectorizer()\n",
    "vect.fit(simple_train)\n",
    "simple_train_dtm = vect.transform(simple_train)\n",
    "pd.DataFrame(simple_train_dtm.toarray(), columns=vect.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the [scikit-learn documentation](http://scikit-learn.org/stable/modules/feature_extraction.html#text-feature-extraction):\n",
    "\n",
    "> In this scheme, features and samples are defined as follows:\n",
    "\n",
    "> - Each individual token occurrence frequency (normalized or not) is treated as a **feature**.\n",
    "> - The vector of all the token frequencies for a given document is considered a multivariate **sample**.\n",
    "\n",
    "> A **corpus of documents** can thus be represented by a matrix with **one row per document** and **one column per token** (e.g. word) occurring in the corpus.\n",
    "\n",
    "> We call **vectorization** the general process of turning a collection of text documents into numerical feature vectors. This specific strategy (tokenization, counting and normalization) is called the **Bag of Words** or \"Bag of n-grams\" representation. Documents are described by word occurrences while completely ignoring the relative position information of the words in the document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vectorizing the yelp reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# use CountVectorizer to create document-term matrices from X_train and X_test\n",
    "vect = CountVectorizer()\n",
    "X_train_dtm = vect.fit_transform(X_train)\n",
    "X_test_dtm = vect.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3064, 16825)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# rows are documents, columns are terms (aka \"tokens\" or \"features\")\n",
    "X_train_dtm.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A Multinomial Naive Bayes model for rating prediction\n",
    "\n",
    "We will use [Multinomial Naive Bayes](http://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html):\n",
    "\n",
    "> The multinomial Naive Bayes classifier is suitable for classification with **discrete features** (e.g., word counts for text classification). The multinomial distribution normally requires integer feature counts. However, in practice, fractional counts such as tf-idf may also work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define a function that accepts a vectorizer\n",
    "# trains a MultinomialNB, and calculates the accuracy\n",
    "def tokenize_test(vect):\n",
    "    X_train_dtm = vect.fit_transform(X_train)\n",
    "    print('Features: ', X_train_dtm.shape[1])\n",
    "    X_test_dtm = vect.transform(X_test)\n",
    "    nb = MultinomialNB()\n",
    "    nb.fit(X_train_dtm, y_train)\n",
    "    y_pred_class = nb.predict(X_test_dtm)\n",
    "    print('Accuracy: ', metrics.accuracy_score(y_test, y_pred_class))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features:  16825\n",
      "Accuracy:  0.918786692759\n"
     ]
    }
   ],
   "source": [
    "tokenize_test(vect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.81996086105675148"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate null accuracy\n",
    "y_test_binary = np.where(y_test==5, 1, 0)\n",
    "max(y_test_binary.mean(), 1 - y_test_binary.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Include $2$-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['zucchini very',\n",
       " 'zucchini with',\n",
       " 'zuchinni',\n",
       " 'zuchinni again',\n",
       " 'zuchinni the',\n",
       " 'zumba',\n",
       " 'zumba class',\n",
       " 'zumba or',\n",
       " 'zumba yogalates',\n",
       " 'zupa',\n",
       " 'zupa flavors',\n",
       " 'zuzu',\n",
       " 'zuzu in',\n",
       " 'zuzu is',\n",
       " 'zuzu the',\n",
       " 'zwiebel',\n",
       " 'zwiebel kräuter',\n",
       " 'zzed',\n",
       " 'zzed in',\n",
       " 'éclairs',\n",
       " 'éclairs napoleons',\n",
       " 'école',\n",
       " 'école lenôtre',\n",
       " 'ém',\n",
       " 'ém all']"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# include 1-grams and 2-grams\n",
    "vect = CountVectorizer(ngram_range=(1, 2))\n",
    "vect.fit(X_train).get_feature_names()[-25:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features:  169847\n",
      "Accuracy:  0.854207436399\n"
     ]
    }
   ],
   "source": [
    "tokenize_test(vect)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**: Why do you think the accuracy went down after adding 2-grams?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stopword Removal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **What:** Remove common words that will likely appear in any text\n",
    "- **Why:** They don't tell you much about your text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features:  16528\n",
      "Accuracy:  0.915851272016\n"
     ]
    }
   ],
   "source": [
    "# remove English stop words\n",
    "vect = CountVectorizer(stop_words='english')\n",
    "tokenize_test(vect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "frozenset({'might', 'therefore', 'himself', 'their', 'detail', 'whither', 'anything', 'indeed', 'yourselves', 'ie', 'they', 'amoungst', 'thus', 'what', 'becoming', 'toward', 'anyhow', 'down', 'hasnt', 'least', 'anyone', 'inc', 'mostly', 'onto', 'seemed', 'sometimes', 'whenever', 'whereas', 'whether', 'a', 'latter', 'besides', 'your', 'six', 'show', 'thereafter', 'back', 'empty', 'very', 'through', 'own', 'neither', 'we', 'can', 'around', 'former', 'against', 'beyond', 'hereupon', 'cant', 'become', 'see', 'both', 'while', 'next', 'per', 'yours', 'of', 'thru', 'is', 'nowhere', 'several', 'off', 'done', 'well', 'yet', 'every', 'else', 'fifteen', 'fify', 'our', 'side', 'between', 'forty', 'few', 'again', 'upon', 'whom', 'ever', 'except', 'otherwise', 'latterly', 'within', 'into', 'than', 'among', 'not', 'moreover', 'somehow', 'must', 'still', 'across', 'go', 'thence', 'perhaps', 'whose', 'however', 'seem', 'became', 'twelve', 'any', 'anyway', 'has', 'herein', 'here', 'ourselves', 'this', 'those', 'enough', 'name', 'some', 'beside', 'how', 'eleven', 'ten', 'and', 'could', 'front', 'thereby', 'then', 'throughout', 'whereupon', 'above', 'put', 'con', 'four', 'mine', 'so', 'thereupon', 'almost', 'seeming', 'her', 'de', 'may', 'us', 'my', 'she', 'he', 'though', 'yourself', 'such', 'are', 'cry', 'beforehand', 'less', 'rather', 'get', 'hereby', 'eg', 'sincere', 'something', 'to', 'bottom', 'although', 'only', 'when', 'nor', 'part', 'if', 'find', 'or', 'but', 'below', 'etc', 'together', 'after', 'had', 'before', 'that', 'which', 'take', 'due', 'couldnt', 'top', 'been', 'for', 'keep', 'hence', 'mill', 'via', 'without', 'with', 'each', 'whereafter', 'was', 'amongst', 'another', 'someone', 'meanwhile', 'will', 'all', 'co', 'there', 'towards', 'over', 'were', 're', 'cannot', 'ours', 'other', 'by', 'whereby', 'from', 'eight', 'call', 'about', 'thick', 'further', 'nine', 'always', 'during', 'wherein', 'either', 'who', 'these', 'hers', 'last', 'sometime', 'thin', 'even', 'fill', 'an', 'already', 'out', 'elsewhere', 'anywhere', 'found', 'where', 'two', 'since', 'whence', 'afterwards', 'under', 'being', 'at', 'it', 'along', 'wherever', 'whoever', 'him', 'formerly', 'whole', 'once', 'hereafter', 'everywhere', 'describe', 'full', 'themselves', 'do', 'somewhere', 'never', 'why', 'until', 'its', 'third', 'now', 'system', 'un', 'more', 'seems', 'up', 'his', 'as', 'in', 'nothing', 'too', 'you', 'others', 'would', 'most', 'move', 'should', 'alone', 'becomes', 'everything', 'because', 'myself', 'whatever', 'nobody', 'the', 'please', 'no', 'itself', 'three', 'first', 'much', 'everyone', 'hundred', 'made', 'many', 'nevertheless', 'twenty', 'amount', 'herself', 'behind', 'namely', 'me', 'give', 'noone', 'ltd', 'sixty', 'five', 'am', 'be', 'fire', 'have', 'i', 'none', 'on', 'often', 'interest', 'also', 'same', 'therein', 'them', 'bill', 'one', 'serious'})\n"
     ]
    }
   ],
   "source": [
    "# set of stop words\n",
    "print(vect.get_stop_words())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming / Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Stemming:**\n",
    "\n",
    "- **What:** Reduce a word to its base/stem/root form\n",
    "- **Why:** Often makes sense to treat related words the same way\n",
    "- **Notes:**\n",
    "    - Uses a \"simple\" and fast rule-based approach\n",
    "    - Stemmed words are usually not shown to users (used for analysis/indexing)\n",
    "    - Some search engines treat words with the same stem as synonyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# grab the first review\n",
    "review = TextBlob(yelp_best_worst.text[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My my\n",
      "wife wife\n",
      "took took\n",
      "me me\n",
      "here here\n",
      "on on\n",
      "my my\n",
      "birthday birthday\n",
      "for for\n",
      "breakfast breakfast\n",
      "and and\n",
      "it it\n",
      "was was\n",
      "excellent excel\n",
      "The the\n",
      "weather weather\n",
      "was was\n",
      "perfect perfect\n",
      "which which\n",
      "made made\n",
      "sitting sit\n",
      "outside outsid\n",
      "overlooking overlook\n",
      "their their\n",
      "grounds ground\n",
      "an an\n",
      "absolute absolut\n",
      "pleasure pleasur\n",
      "Our our\n",
      "waitress waitress\n",
      "was was\n",
      "excellent excel\n",
      "and and\n",
      "our our\n",
      "food food\n",
      "arrived arriv\n",
      "quickly quick\n",
      "on on\n",
      "the the\n",
      "semi-busy semi-busi\n",
      "Saturday saturday\n",
      "morning morn\n",
      "It it\n",
      "looked look\n",
      "like like\n",
      "the the\n",
      "place place\n",
      "fills fill\n",
      "up up\n",
      "pretty pretti\n",
      "quickly quick\n",
      "so so\n",
      "the the\n",
      "earlier earlier\n",
      "you you\n",
      "get get\n",
      "here here\n",
      "the the\n",
      "better better\n",
      "Do do\n",
      "yourself yourself\n",
      "a a\n",
      "favor favor\n",
      "and and\n",
      "get get\n",
      "their their\n",
      "Bloody bloodi\n",
      "Mary mari\n",
      "It it\n",
      "was was\n",
      "phenomenal phenomen\n",
      "and and\n",
      "simply simpli\n",
      "the the\n",
      "best best\n",
      "I i\n",
      "'ve ve\n",
      "ever ever\n",
      "had had\n",
      "I i\n",
      "'m 'm\n",
      "pretty pretti\n",
      "sure sure\n",
      "they they\n",
      "only onli\n",
      "use use\n",
      "ingredients ingredi\n",
      "from from\n",
      "their their\n",
      "garden garden\n",
      "and and\n",
      "blend blend\n",
      "them them\n",
      "fresh fresh\n",
      "when when\n",
      "you you\n",
      "order order\n",
      "it it\n",
      "It it\n",
      "was was\n",
      "amazing amaz\n",
      "While while\n",
      "EVERYTHING everyth\n",
      "on on\n",
      "the the\n",
      "menu menu\n",
      "looks look\n",
      "excellent excel\n",
      "I i\n",
      "had had\n",
      "the the\n",
      "white white\n",
      "truffle truffl\n",
      "scrambled scrambl\n",
      "eggs egg\n",
      "vegetable veget\n",
      "skillet skillet\n",
      "and and\n",
      "it it\n",
      "was was\n",
      "tasty tasti\n",
      "and and\n",
      "delicious delici\n",
      "It it\n",
      "came came\n",
      "with with\n",
      "2 2\n",
      "pieces piec\n",
      "of of\n",
      "their their\n",
      "griddled griddl\n",
      "bread bread\n",
      "with with\n",
      "was was\n",
      "amazing amaz\n",
      "and and\n",
      "it it\n",
      "absolutely absolut\n",
      "made made\n",
      "the the\n",
      "meal meal\n",
      "complete complet\n",
      "It it\n",
      "was was\n",
      "the the\n",
      "best best\n",
      "toast toast\n",
      "I i\n",
      "'ve ve\n",
      "ever ever\n",
      "had had\n",
      "Anyway anyway\n",
      "I i\n",
      "ca ca\n",
      "n't n't\n",
      "wait wait\n",
      "to to\n",
      "go go\n",
      "back back\n"
     ]
    }
   ],
   "source": [
    "# initialize stemmer\n",
    "stemmer = SnowballStemmer('english')\n",
    "\n",
    "# stem each word\n",
    "for orig, stemmed in zip(review.words, [stemmer.stem(word) for word in review.words]):\n",
    "    print(orig, stemmed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Lemmatization**\n",
    "\n",
    "- **What:** Derive the canonical form ('lemma') of a word\n",
    "- **Why:** Can be better than stemming\n",
    "- **Notes:** Uses a dictionary-based approach (slower than stemming)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define a function that accepts text and returns a list of lemmas\n",
    "def split_into_lemmas(text):\n",
    "    text = text.lower()\n",
    "    words = TextBlob(text).words\n",
    "    return [word.lemmatize() for word in words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My my\n",
      "wife wife\n",
      "took took\n",
      "me me\n",
      "here here\n",
      "on on\n",
      "my my\n",
      "birthday birthday\n",
      "for for\n",
      "breakfast breakfast\n",
      "and and\n",
      "it it\n",
      "was wa\n",
      "excellent excellent\n",
      "The the\n",
      "weather weather\n",
      "was wa\n",
      "perfect perfect\n",
      "which which\n",
      "made made\n",
      "sitting sitting\n",
      "outside outside\n",
      "overlooking overlooking\n",
      "their their\n",
      "grounds ground\n",
      "an an\n",
      "absolute absolute\n",
      "pleasure pleasure\n",
      "Our our\n",
      "waitress waitress\n",
      "was wa\n",
      "excellent excellent\n",
      "and and\n",
      "our our\n",
      "food food\n",
      "arrived arrived\n",
      "quickly quickly\n",
      "on on\n",
      "the the\n",
      "semi-busy semi-busy\n",
      "Saturday saturday\n",
      "morning morning\n",
      "It it\n",
      "looked looked\n",
      "like like\n",
      "the the\n",
      "place place\n",
      "fills fill\n",
      "up up\n",
      "pretty pretty\n",
      "quickly quickly\n",
      "so so\n",
      "the the\n",
      "earlier earlier\n",
      "you you\n",
      "get get\n",
      "here here\n",
      "the the\n",
      "better better\n",
      "Do do\n",
      "yourself yourself\n",
      "a a\n",
      "favor favor\n",
      "and and\n",
      "get get\n",
      "their their\n",
      "Bloody bloody\n",
      "Mary mary\n",
      "It it\n",
      "was wa\n",
      "phenomenal phenomenal\n",
      "and and\n",
      "simply simply\n",
      "the the\n",
      "best best\n",
      "I i\n",
      "'ve 've\n",
      "ever ever\n",
      "had had\n",
      "I i\n",
      "'m 'm\n",
      "pretty pretty\n",
      "sure sure\n",
      "they they\n",
      "only only\n",
      "use use\n",
      "ingredients ingredient\n",
      "from from\n",
      "their their\n",
      "garden garden\n",
      "and and\n",
      "blend blend\n",
      "them them\n",
      "fresh fresh\n",
      "when when\n",
      "you you\n",
      "order order\n",
      "it it\n",
      "It it\n",
      "was wa\n",
      "amazing amazing\n",
      "While while\n",
      "EVERYTHING everything\n",
      "on on\n",
      "the the\n",
      "menu menu\n",
      "looks look\n",
      "excellent excellent\n",
      "I i\n",
      "had had\n",
      "the the\n",
      "white white\n",
      "truffle truffle\n",
      "scrambled scrambled\n",
      "eggs egg\n",
      "vegetable vegetable\n",
      "skillet skillet\n",
      "and and\n",
      "it it\n",
      "was wa\n",
      "tasty tasty\n",
      "and and\n",
      "delicious delicious\n",
      "It it\n",
      "came came\n",
      "with with\n",
      "2 2\n",
      "pieces piece\n",
      "of of\n",
      "their their\n",
      "griddled griddled\n",
      "bread bread\n",
      "with with\n",
      "was wa\n",
      "amazing amazing\n",
      "and and\n",
      "it it\n",
      "absolutely absolutely\n",
      "made made\n",
      "the the\n",
      "meal meal\n",
      "complete complete\n",
      "It it\n",
      "was wa\n",
      "the the\n",
      "best best\n",
      "toast toast\n",
      "I i\n",
      "'ve 've\n",
      "ever ever\n",
      "had had\n",
      "Anyway anyway\n",
      "I i\n",
      "ca ca\n",
      "n't n't\n",
      "wait wait\n",
      "to to\n",
      "go go\n",
      "back back\n"
     ]
    }
   ],
   "source": [
    "# print words with lemmatized versions\n",
    "for orig, stemmed in zip(review.words, split_into_lemmas(review.string)):\n",
    "    print(orig, stemmed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features:  16452\n",
      "Accuracy:  0.920743639922\n"
     ]
    }
   ],
   "source": [
    "# use split_into_lemmas as the feature extraction function (WARNING: SLOW!)\n",
    "vect = CountVectorizer(analyzer=split_into_lemmas)\n",
    "tokenize_test(vect)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A marginal 0.2% improvement over no preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Term Frequency-Inverse Document Frequency (TF-IDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **What:** Computes \"relative frequency\" that a word appears in a document compared to its frequency across all documents\n",
    "- **Why:** More useful than \"term frequency\" for identifying \"important\" words in each document (high frequency in that document, low frequency in other documents)\n",
    "- **Notes:** Used for search engine scoring, text summarization, document clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# example documents\n",
    "simple_train = ['call you tonight', 'Call me a cab', 'please call me... PLEASE!']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cab</th>\n",
       "      <th>call</th>\n",
       "      <th>me</th>\n",
       "      <th>please</th>\n",
       "      <th>tonight</th>\n",
       "      <th>you</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   cab  call  me  please  tonight  you\n",
       "0    0     1   0       0        1    1\n",
       "1    1     1   1       0        0    0\n",
       "2    0     1   1       2        0    0"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Term Frequency\n",
    "vect = CountVectorizer()\n",
    "tf = pd.DataFrame(vect.fit_transform(simple_train).toarray(), columns=vect.get_feature_names())\n",
    "tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cab</th>\n",
       "      <th>call</th>\n",
       "      <th>me</th>\n",
       "      <th>please</th>\n",
       "      <th>tonight</th>\n",
       "      <th>you</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   cab  call  me  please  tonight  you\n",
       "0    1     3   2       1        1    1"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Document Frequency\n",
    "vect = CountVectorizer(binary=True)\n",
    "df = vect.fit_transform(simple_train).toarray().sum(axis=0)\n",
    "pd.DataFrame(df.reshape(1, 6), columns=vect.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cab</th>\n",
       "      <th>call</th>\n",
       "      <th>me</th>\n",
       "      <th>please</th>\n",
       "      <th>tonight</th>\n",
       "      <th>you</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.5</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   cab      call   me  please  tonight  you\n",
       "0  0.0  0.333333  0.0     0.0      1.0  1.0\n",
       "1  1.0  0.333333  0.5     0.0      0.0  0.0\n",
       "2  0.0  0.333333  0.5     2.0      0.0  0.0"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Term Frequency-Inverse Document Frequency (simple version)\n",
    "(tf/df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cab</th>\n",
       "      <th>call</th>\n",
       "      <th>me</th>\n",
       "      <th>please</th>\n",
       "      <th>tonight</th>\n",
       "      <th>you</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.385372</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.652491</td>\n",
       "      <td>0.652491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.720333</td>\n",
       "      <td>0.425441</td>\n",
       "      <td>0.547832</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.266075</td>\n",
       "      <td>0.342620</td>\n",
       "      <td>0.901008</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        cab      call        me    please   tonight       you\n",
       "0  0.000000  0.385372  0.000000  0.000000  0.652491  0.652491\n",
       "1  0.720333  0.425441  0.547832  0.000000  0.000000  0.000000\n",
       "2  0.000000  0.266075  0.342620  0.901008  0.000000  0.000000"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TfidfVectorizer\n",
    "vect = TfidfVectorizer()\n",
    "pd.DataFrame(vect.fit_transform(simple_train).toarray(), columns=vect.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**More details:** [TF-IDF is about what matters](http://planspace.org/20150524-tfidf_is_about_what_matters/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using TF-IDF to Summarize a Yelp Review\n",
    "\n",
    "Reddit's autotldr uses the [SMMRY](http://smmry.com/about) algorithm, which is based on TF-IDF!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 28881)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a document-term matrix using TF-IDF\n",
    "vect = TfidfVectorizer(stop_words='english')\n",
    "dtm = vect.fit_transform(yelp.text)\n",
    "features = vect.get_feature_names()\n",
    "dtm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def summarize():\n",
    "    # choose a random review that is at least 300 characters\n",
    "    review_length = 0\n",
    "    while review_length < 300:\n",
    "        review_id = np.random.randint(0, len(yelp))\n",
    "        review_text = yelp.text[review_id]\n",
    "        review_length = len(review_text)\n",
    "    \n",
    "    # create a dictionary of words and their TF-IDF scores\n",
    "    word_scores = {}\n",
    "    for word in TextBlob(review_text).words:\n",
    "        word = word.lower()\n",
    "        if word in features:\n",
    "            word_scores[word] = dtm[review_id, features.index(word)]\n",
    "    \n",
    "    # print words with the top 5 TF-IDF scores\n",
    "    print('TOP SCORING WORDS:')\n",
    "    top_scores = sorted(word_scores.items(), key=lambda x: x[1], reverse=True)[:5]\n",
    "    for word, score in top_scores:\n",
    "        print(word)\n",
    "    \n",
    "    # print 5 random words\n",
    "    print('\\n' + 'RANDOM WORDS:')\n",
    "    random_words = np.random.choice(list(word_scores.keys()), size=5, replace=False)\n",
    "    for word in random_words:\n",
    "        print(word)\n",
    "    \n",
    "    # print the review\n",
    "    print('\\n' + review_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOP SCORING WORDS:\n",
      "margherita\n",
      "biancoverde\n",
      "incredible\n",
      "rating\n",
      "hair\n",
      "\n",
      "RANDOM WORDS:\n",
      "replace\n",
      "neopolitana\n",
      "lunch\n",
      "flavors\n",
      "america\n",
      "\n",
      "I was extremely excited to try what is considered some of the \"best pizza\" in America..., finding out it was only minutes away from my new office. My expectations may have been too high. Our party of four went for a late lunch, we did not have to wait for a table. \n",
      "\n",
      "My fiance and I decided to try 2 pies, the margherita and the Biancoverde. The Margherita pie came out quite dry, not with the wonderful chewy crust that a great margherita should have... however, the Biancoverde, a white pizza with Ricotta and arugala, was fantastic! Incredible flavors, the extra toppings kept this pie perfectly moist and delicious, it was really quite perfect. \n",
      "Unfortunately, we had a hair in one of our pizzas, however, that's only part of the reason for the mediocre rating... \n",
      "The hair, the 1-2 star margherita, and the non-existant meat selection I can only give a 3 star rating... (I'm a bit of a carnivore, I need more meat!)\n",
      "I'm still looking for something to replace the incredible pizzas I've had at my favorite neopolitana joing back home in Canada, Famoso. \n",
      "\n",
      "Will I be back to Pizzeria Bianco for another go? Absolutely...\n"
     ]
    }
   ],
   "source": [
    "summarize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My wife took me here on my birthday for breakfast and it was excellent.  The weather was perfect which made sitting outside overlooking their grounds an absolute pleasure.  Our waitress was excellent and our food arrived quickly on the semi-busy Saturday morning.  It looked like the place fills up pretty quickly so the earlier you get here the better.\n",
      "\n",
      "Do yourself a favor and get their Bloody Mary.  It was phenomenal and simply the best I've ever had.  I'm pretty sure they only use ingredients from their garden and blend them fresh when you order it.  It was amazing.\n",
      "\n",
      "While EVERYTHING on the menu looks excellent, I had the white truffle scrambled eggs vegetable skillet and it was tasty and delicious.  It came with 2 pieces of their griddled bread with was amazing and it absolutely made the meal complete.  It was the best \"toast\" I've ever had.\n",
      "\n",
      "Anyway, I can't wait to go back!\n"
     ]
    }
   ],
   "source": [
    "print(review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.40246913580246907"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# provided by `TextBlob`\n",
    "# polarity ranges from -1 (most negative) to 1 (most positive)\n",
    "review.sentiment.polarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# define a function that accepts text and returns the polarity\n",
    "def detect_sentiment(text):\n",
    "    return TextBlob(text).sentiment.polarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create a new DataFrame column for sentiment (WARNING: SLOW!)\n",
    "yelp['sentiment'] = yelp.text.apply(detect_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEcCAYAAADKlrO6AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nwv1/tp8fUlOwZ2kAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fe8e99abb00>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# box plot of sentiment grouped by stars\n",
    "yelp.boxplot(column='sentiment', by='stars')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "254    Our server Gary was awesome. Food was amazing....\n",
       "347    3 syllables for this place. \\nA-MAZ-ING!\\n\\nTh...\n",
       "420                                    LOVE the food!!!!\n",
       "459    Love it!!! Wish we still lived in Arizona as C...\n",
       "679                                     Excellent burger\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reviews with most positive sentiment\n",
    "yelp[yelp.sentiment == 1].text.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "773     This was absolutely horrible. I got the suprem...\n",
       "1517                  Nasty workers and over priced trash\n",
       "3266    Absolutely awful... these guys have NO idea wh...\n",
       "4766                                       Very bad food!\n",
       "5812        I wouldn't send my worst enemy to this place.\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reviews with most negative sentiment\n",
    "yelp[yelp.sentiment == -1].text.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# widen the column display\n",
    "pd.set_option('max_colwidth', 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>business_id</th>\n",
       "      <th>date</th>\n",
       "      <th>review_id</th>\n",
       "      <th>stars</th>\n",
       "      <th>text</th>\n",
       "      <th>type</th>\n",
       "      <th>user_id</th>\n",
       "      <th>cool</th>\n",
       "      <th>useful</th>\n",
       "      <th>funny</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>390</th>\n",
       "      <td>106JT5p8e8Chtd0CZpcARw</td>\n",
       "      <td>2009-08-06</td>\n",
       "      <td>KowGVoP_gygzdSu6Mt3zKQ</td>\n",
       "      <td>5</td>\n",
       "      <td>RIP AZ Coffee Connection.  :(  I stopped by two days ago unaware that they had closed.  I am severely bummed.  This place is irreplaceable!  Damn you, Starbucks and McDonalds!</td>\n",
       "      <td>review</td>\n",
       "      <td>jKeaOrPyJ-dI9SNeVqrbww</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.302083</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                business_id        date               review_id  stars  \\\n",
       "390  106JT5p8e8Chtd0CZpcARw  2009-08-06  KowGVoP_gygzdSu6Mt3zKQ      5   \n",
       "\n",
       "                                                                                                                                                                                text  \\\n",
       "390  RIP AZ Coffee Connection.  :(  I stopped by two days ago unaware that they had closed.  I am severely bummed.  This place is irreplaceable!  Damn you, Starbucks and McDonalds!   \n",
       "\n",
       "       type                 user_id  cool  useful  funny  sentiment  \n",
       "390  review  jKeaOrPyJ-dI9SNeVqrbww     1       0      0  -0.302083  "
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# negative sentiment in a 5-star review\n",
    "yelp[(yelp.stars == 5) & (yelp.sentiment < -0.3)].head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>business_id</th>\n",
       "      <th>date</th>\n",
       "      <th>review_id</th>\n",
       "      <th>stars</th>\n",
       "      <th>text</th>\n",
       "      <th>type</th>\n",
       "      <th>user_id</th>\n",
       "      <th>cool</th>\n",
       "      <th>useful</th>\n",
       "      <th>funny</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1781</th>\n",
       "      <td>53YGfwmbW73JhFiemNeyzQ</td>\n",
       "      <td>2012-06-22</td>\n",
       "      <td>Gi-4O3EhE175vujbFGDIew</td>\n",
       "      <td>1</td>\n",
       "      <td>If you like the stuck up Scottsdale vibe this is a good place for you. The food isn't impressive. Nice outdoor seating.</td>\n",
       "      <td>review</td>\n",
       "      <td>Hqgx3IdJAAaoQjvrUnbNvw</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.766667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 business_id        date               review_id  stars  \\\n",
       "1781  53YGfwmbW73JhFiemNeyzQ  2012-06-22  Gi-4O3EhE175vujbFGDIew      1   \n",
       "\n",
       "                                                                                                                         text  \\\n",
       "1781  If you like the stuck up Scottsdale vibe this is a good place for you. The food isn't impressive. Nice outdoor seating.   \n",
       "\n",
       "        type                 user_id  cool  useful  funny  sentiment  \n",
       "1781  review  Hqgx3IdJAAaoQjvrUnbNvw     0       1      2   0.766667  "
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# positive sentiment in a 1-star review\n",
    "yelp[(yelp.stars == 1) & (yelp.sentiment > 0.5)].head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# reset the column display width\n",
    "pd.reset_option('max_colwidth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
